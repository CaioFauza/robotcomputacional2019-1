{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entregável 3 de Visão Computacional e Robótica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrega até 12/03 ao fim do atendimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode ser feito **em duplas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta semana vamos trabalhar com um assunto extremamente atual: reconhecimento de objetos e rastreamento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referências:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/](https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/)\n",
    "\n",
    "[https://github.com/iArunava/YOLOv3-Object-Detection-with-OpenCV/](https://github.com/iArunava/YOLOv3-Object-Detection-with-OpenCV/)\n",
    "\n",
    "[https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/](https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouça a explicacão do professor sobre rastreamento e deteção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Executar os três exemplos\n",
    "\n",
    "Há três exemplos: `mobilenet_detection`, `yolov3_detection` e `tracking`.\n",
    "\n",
    "Os dois primeiros são reconhecedores de objetos, e o último é de rastreamento\n",
    "\n",
    "\n",
    "Um dos arquivos abaixo precisa ser baixado e salvo nas pasta  `yolov3_detection/yolov3-coco` .\n",
    "\n",
    "[https://www.dropbox.com/s/013ogt2bhwfzxwb/yolov3.weights?dl=0](https://www.dropbox.com/s/013ogt2bhwfzxwb/yolov3.weights?dl=0) ou [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identificar objeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolha um dos projetos: `yolov3_detection` ou `mobilenet_detection` para basear seu código. \n",
    "\n",
    "Neste projeto, escolha uma categoria de objetos que o reconhecedor reconhece. Diga aqui qual foi sua escolha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3>Projeto escolhido: yolov3_detection </h3></center>\n",
    "<center><h3> Categoria escolhida: bottle </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rastrear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente a seguinte funcionalidade: sempre que o objeto identificado em (2) estiver presente por mais que 5 frames seguidos, pare de rodar a identificação e comece a rastreá-lo.\n",
    "\n",
    "Faça um vídeo com a  demonstração funcionalidade e mostre o link ao professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from yolo_utils import infer_image, show_image\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "\n",
    "FLAGS = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Adapted from:\n",
    "\n",
    "https://github.com/iArunava/YOLOv3-Object-Detection-with-OpenCV/blob/master/yolo.py#L154\n",
    "\n",
    "\"\"\"\n",
    "def print_categories(boxes, confidences, classids, labels):\n",
    "\tlista = list()\n",
    "\t\"\"\" \n",
    "\tNota do Miranda: \n",
    "\t\tFunção estratégica para modificar/ usar o Yolo\n",
    "\t\"\"\"\n",
    "\tfor i in range(len(classids)):\n",
    "\t\tlista.append(labels[classids[i]])\n",
    "\treturn lista\n",
    "\n",
    "\n",
    "def caixa(boxes, classids):\n",
    "\tlistaboxes = list()\n",
    "\tfor i in range(len(classids)):\n",
    "\t\tlistaboxes.append(boxes[i])\n",
    "\treturn listaboxes[0]\n",
    "\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "########################################################################## TRACKING ##########################################################################\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-v\", \"--video\", type=str,\n",
    "\thelp=\"path to input video file\")\n",
    "ap.add_argument(\"-t\", \"--tracker\", type=str, default=\"kcf\",\n",
    "\thelp=\"OpenCV object tracker type\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# extract the OpenCV version info\n",
    "(major, minor) = cv2.__version__.split(\".\")[:2]\n",
    "\n",
    "# if we are using OpenCV 3.2 OR BEFORE, we can use a special factory\n",
    "# function to create our object tracker\n",
    "if int(major) == 3 and int(minor) < 3:\n",
    "\ttracker = cv2.Tracker_create(args[\"tracker\"].upper())\n",
    "\n",
    "# otherwise, for OpenCV 3.3 OR NEWER, we need to explicity call the\n",
    "# approrpiate object tracker constructor:\n",
    "else:\n",
    "\t# initialize a dictionary that maps strings to their corresponding\n",
    "\t# OpenCV object tracker implementations\n",
    "\tOPENCV_OBJECT_TRACKERS = {\n",
    "\t\t\"csrt\": cv2.TrackerCSRT_create,\n",
    "\t\t\"kcf\": cv2.TrackerKCF_create,\n",
    "\t\t\"boosting\": cv2.TrackerBoosting_create,\n",
    "\t\t\"mil\": cv2.TrackerMIL_create,\n",
    "\t\t\"tld\": cv2.TrackerTLD_create,\n",
    "\t\t\"medianflow\": cv2.TrackerMedianFlow_create,\n",
    "\t\t\"mosse\": cv2.TrackerMOSSE_create\n",
    "\t}\n",
    "\n",
    "\t# grab the appropriate object tracker using our dictionary of\n",
    "\t# OpenCV object tracker objects\n",
    "\ttracker = OPENCV_OBJECT_TRACKERS[args[\"tracker\"]]()\n",
    "\n",
    "# initialize the bounding box coordinates of the object we are going\n",
    "# to track\n",
    "objeto = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\n",
    "\tparser.add_argument('-m', '--model-path',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/',\n",
    "\t\thelp='The directory where the model weights and \\\n",
    "\t\t\t  configuration files are.')\n",
    "\n",
    "\tparser.add_argument('-w', '--weights',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/yolov3.weights',\n",
    "\t\thelp='Path to the file which contains the weights \\\n",
    "\t\t\t \tfor YOLOv3.')\n",
    "\n",
    "\tparser.add_argument('-cfg', '--config',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/yolov3.cfg',\n",
    "\t\thelp='Path to the configuration file for the YOLOv3 model.')\n",
    "\n",
    "\tparser.add_argument('-i', '--image-path',\n",
    "\t\ttype=str,\n",
    "\t\thelp='The path to the image file')\n",
    "\n",
    "\tparser.add_argument('-v', '--video-path',\n",
    "\t\ttype=str,\n",
    "\t\thelp='The path to the video file')\n",
    "\n",
    "\n",
    "\tparser.add_argument('-vo', '--video-output-path',\n",
    "\t\ttype=str,\n",
    "        default='./output.avi',\n",
    "\t\thelp='The path of the output video file')\n",
    "\n",
    "\tparser.add_argument('-l', '--labels',\n",
    "\t\ttype=str,\n",
    "\t\tdefault='./yolov3-coco/coco-labels',\n",
    "\t\thelp='Path to the file having the \\\n",
    "\t\t\t\t\tlabels in a new-line seperated way.')\n",
    "\n",
    "\tparser.add_argument('-c', '--confidence',\n",
    "\t\ttype=float,\n",
    "\t\tdefault=0.5,\n",
    "\t\thelp='The model will reject boundaries which has a \\\n",
    "\t\t\t\tprobabiity less than the confidence value. \\\n",
    "\t\t\t\tdefault: 0.5')\n",
    "\n",
    "\tparser.add_argument('-th', '--threshold',\n",
    "\t\ttype=float,\n",
    "\t\tdefault=0.3,\n",
    "\t\thelp='The threshold to use when applying the \\\n",
    "\t\t\t\tNon-Max Suppresion')\n",
    "\n",
    "\tparser.add_argument('--download-model',\n",
    "\t\ttype=bool,\n",
    "\t\tdefault=False,\n",
    "\t\thelp='Set to True, if the model weights and configurations \\\n",
    "\t\t\t\tare not present on your local machine.')\n",
    "\n",
    "\tparser.add_argument('-t', '--show-time',\n",
    "\t\ttype=bool,\n",
    "\t\tdefault=False,\n",
    "\t\thelp='Show the time taken to infer each image.')\n",
    "\n",
    "\tFLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "\t# Download the YOLOv3 models if needed\n",
    "\tif FLAGS.download_model:\n",
    "\t\tsubprocess.call(['./yolov3-coco/get_model.sh'])\n",
    "\n",
    "\t# Get the labels\n",
    "\tlabels = open(FLAGS.labels).read().strip().split('\\n')\n",
    "\n",
    "\t# Known categories\n",
    "\tprint(\"Known categories: \", \" \".join(labels))\n",
    "\n",
    "\t# Intializing colors to represent each label uniquely\n",
    "\tcolors = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "\t# Load the weights and configutation to form the pretrained YOLOv3 model\n",
    "\tnet = cv2.dnn.readNetFromDarknet(FLAGS.config, FLAGS.weights)\n",
    "\n",
    "\t# Get the output layer names of the model\n",
    "\tlayer_names = net.getLayerNames()\n",
    "\tlayer_names = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "        \n",
    "\n",
    "\n",
    "\t\n",
    "\n",
    "\tcount = 0\n",
    "\tbottlecounter = 0\n",
    "\tfps = None\n",
    "\n",
    "\twhile True:\n",
    "\t\tret, frame = vid.read()\n",
    "\t\tif ret != False:\n",
    "\t\t\tframe = imutils.resize(frame, width=600)\n",
    "\n",
    "\n",
    "\t\t\theight, width = frame.shape[:2]\n",
    "\n",
    "\t\t\tif bottlecounter <5 :\n",
    "\t\t\t\tif count == 0:\n",
    "\t\t\t\t\tframe, boxes, confidences, classids, idxs = infer_image(net, layer_names, \\\n",
    "\t\t\t\t\t\t\t\t\t\theight, width, frame, colors, labels, FLAGS)\n",
    "\t\t\t\t\tcount += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tframe, boxes, confidences, classids, idxs = infer_image(net, layer_names, \\\n",
    "\t\t\t\t\t\t\t\t\t\theight, width, frame, colors, labels, FLAGS, boxes, confidences, classids, idxs, infer=False)\n",
    "\t\t\t\t\tcount = (count + 1) % 6\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "\t\t\tif any(\"bottle\" in i for i in print_categories(boxes, confidences, classids, labels)):\n",
    "\t\t\t\tbottlecounter += 1\n",
    "\n",
    "\n",
    "\t\t\tif bottlecounter >= 5:\n",
    "\t\t\t\tobjeto = tuple(caixa(boxes, classids))\n",
    "\t\t\t\tif objeto is not None:\n",
    "\t\t\t\t\ttracker.init(frame, objeto)\n",
    "\t\t\t\t\tfps = FPS().start()\n",
    "\t\t\t\t\t# grab the new bounding box coordinates of the object\n",
    "\t\t\t\t\t(success, box) = tracker.update(frame)\n",
    "\n",
    "\t\t\t\t\t# check to see if the tracking was a success\n",
    "\t\t\t\t\tif success:\n",
    "\t\t\t\t\t\t(x, y, w, h) = [int(v) for v in box]\n",
    "\t\t\t\t\t\tcv2.rectangle(frame, (x, y), (x + w, y + h),\n",
    "\t\t\t\t\t\t\t(0, 255, 0), 2)\n",
    "\n",
    "\t\t\t\t\t# update the FPS counter\n",
    "\t\t\t\t\tfps.update()\n",
    "\t\t\t\t\tfps.stop()\n",
    "\n",
    "\t\t\t\t\t# initialize the set of information we'll be displaying on\n",
    "\t\t\t\t\t# the frame\n",
    "\t\t\t\t\tinfo = [\n",
    "\t\t\t\t\t\t(\"Tracker\", args[\"tracker\"]),\n",
    "\t\t\t\t\t\t(\"Success\", \"Yes\" if success else \"No\"),\n",
    "\t\t\t\t\t\t(\"FPS\", \"{:.2f}\".format(fps.fps())),\n",
    "\t\t\t\t\t]\n",
    "\n",
    "\t\t\t\t\t# loop over the info tuples and draw them on our frame\n",
    "\t\t\t\t\tfor (i, (k, v)) in enumerate(info):\n",
    "\t\t\t\t\t\ttext = \"{}: {}\".format(k, v)\n",
    "\t\t\t\t\t\tcv2.putText(frame, text, (10, height - ((i * 20) + 20)),\n",
    "\t\t\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\tcv2.imshow('webcam', frame)\n",
    "\n",
    "\t\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\t\tbreak\n",
    "\tvid.release()\n",
    "\tcv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"tracker.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"tracker.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Rode o simulador do Turtlebot (use o Waffle).  Veja o guia em [../guides/simulador_ros.md](../guides/simulador_ros.md)\n",
    " \n",
    " Documente aqui as linhas necessárias para teleop e para abrir o Rviz\n",
    " \n",
    " Faça um screenshot do seu simulação em execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teleop: rqt_image_view, roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n",
    "#### Rviz: roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch\n",
    "\n",
    "<img src = \"Teleop.png\">\n",
    "<img src= \"Rviz.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Robô quadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça [este tutorial](../guides/projeto_rospython.md) de como criar um projeto Python que comanda o robô simulado.\n",
    "\n",
    "Usando o simulador, crie um código que faça o robô fazer uma trajetória que aproxima um quadrado.\n",
    "\n",
    "Baseie-se no código `roda.py`, construído durante o tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import rospy\n",
    "from geometry_msgs.msg import Twist, Vector3\n",
    "\n",
    "v = 0.1  # Velocidade linear\n",
    "w = 0.4 # Velocidade angular\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rospy.init_node(\"roda_exemplo\")\n",
    "    pub = rospy.Publisher(\"cmd_vel\", Twist, queue_size=3)\n",
    "\n",
    "    try:\n",
    "        while not rospy.is_shutdown():\n",
    "\n",
    "            pub.publish(Twist(Vector3(v, 0, 0), Vector3(0,0,0)))\n",
    "            rospy.sleep(3.0)\n",
    "\n",
    "            pub.publish(Twist(Vector3(0, 0, 0), Vector3(0,0,0)))\n",
    "            rospy.sleep(3.0)\n",
    "\n",
    "            pub.publish(Twist(Vector3(0, 0, 0), Vector3(0,0,w)))\n",
    "            rospy.sleep(4.0)\n",
    "\n",
    "            pub.publish(Twist(Vector3(0, 0, 0), Vector3(0,0,0)))\n",
    "            rospy.sleep(3.0)\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "           \n",
    "    except rospy.ROSInterruptException:\n",
    "        print(\"Ocorreu uma exceção com o rospy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Robô indeciso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o simulador e o LIDAR simulado, faça um robô avançar quando o obstáculo bem à sua frente estiver a menos de 1.0m e recuar quando estiver a mais de 1.02 m.\n",
    "\n",
    "Baseie-se no código `le_scan.py` e `roda.py`, desenvolvidos [durante o tutorial](../guides/projeto_rospython.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "import numpy as np\n",
    "from geometry_msgs.msg import Twist, Vector3\n",
    "from sensor_msgs.msg import LaserScan\n",
    "\n",
    "\n",
    "andar = False\n",
    "d = 100\n",
    "\n",
    "def scaneou(dado):\n",
    "    global d\n",
    " \n",
    "    d = np.amin(np.array(dado.ranges).round(decimals=2))\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    rospy.init_node(\"le_scan\")\n",
    "\n",
    "    velocidade_saida = rospy.Publisher(\"/cmd_vel\", Twist, queue_size = 3 )\n",
    "    recebe_scan = rospy.Subscriber(\"/scan\", LaserScan, scaneou)\n",
    "\n",
    "\n",
    "\n",
    "    while not rospy.is_shutdown():\n",
    "\n",
    "        while not andar:\n",
    "            print(d)\n",
    "        \n",
    "            if d > 1.02:\n",
    "                print(\"Livre\")\n",
    "                velocidade_saida.publish(Twist(Vector3(0,0,0), Vector3(0,0,0)))\n",
    "                rospy.sleep(2.0)\n",
    "\n",
    "                andar = True\n",
    "\n",
    "            else:\n",
    "                velocidade_saida.publish(Twist(Vector3(-0.3, 0, 0), Vector3(0, 0, 0)))\n",
    "                rospy.sleep(1.0) \n",
    "                velocidade_saida.publish(Twist(Vector3(0,0,0), Vector3(0,0,1)))\n",
    "                rospy.sleep(2.0)\n",
    "                velocidade_saida.publish(Twist(Vector3(0.3, 0, 0), Vector3(0, 0, 0)))\n",
    "                rospy.sleep(2)\n",
    "\n",
    "\n",
    "        while andar:\n",
    "\n",
    "            if d > 1.02:\n",
    "                velocidade_saida.publish(Twist(Vector3(0.3, 0, 0), Vector3(0, 0, 0)))\n",
    "                rospy.sleep(2.0)\n",
    "                     \n",
    "\n",
    "            else:\n",
    "                velocidade_saida.publish(Twist(Vector3(0,0,0), Vector3(0,0,0)))\n",
    "                rospy.sleep(2.0)\n",
    "                velocidade_saida.publish(Twist(Vector3(-0.3, 0, 0), Vector3(0, 0, 0)))\n",
    "                rospy.sleep(1.0) \n",
    "                andar  = False \n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
